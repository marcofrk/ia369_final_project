{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Machine Learning Experience With Multimedia Techniques\n",
    "Marco A. Franchi\n",
    "\n",
    "## *Abstract*\n",
    "\n",
    "*It has been very common face machine learning algorithms at the multimedia area: traffic count; real-time vigilance cameras; baggage tracker; face/expression recognition; and so on. It is so common that it was designed a name for it, Machine Vision. However, it is perceptive the gap between machine learning and multimedia solutions, where even at simple embedded systems it is possible to reach 4k videos running at 60 frames per second, whereas the best neural network solution only handles 224x224 frames at 300 milliseconds in the same system. Due to this, a vast number of solutions as being developed: dedicated hardware for inference process; model manipulation; accelerated pre-processing image solutions; and video manipulation techniques. This paper is based at the study of these videos manipulation techniques, exposing the most common algorithmics, such as frame-skip, frame-droop, resizing, color convert, and overlay solutions; and showing the main difference between them, when the solution can be applied, and the expected performance for each one. This paper code can be find in GitHub, which includes a Jupyter Notebook reproducible version.*\n",
    "    https://github.com/marcofrk/ia369_final_project\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Aiming to diminish the gap between the machine learning inference process and the multimedia capability, which reaches 4k@60fps, some video manipulation solutions was purposed. Among them, the most common is the overlay solutions, which are able to create alpha layers over the video and insert information on it. These overlays are very common on object detections algorithmics, once they are responsible for drawing a square, select or color an object at the scene. The most common overlays are Scalable Vector Graphics (SVG), Cairo, and OpenCV.\n",
    "In addition to the overlays, it is important to use a great video framework able to handle all the elements involved at the inference and display solutions. One of the best and most useful ones is the GStreamer framework. GStreamer is able to handle plugins in pipelines, which is perfect to do quick tests.\n",
    "Apart from the video solutions, it is important to choose the best machine learning algorithmics as well. With a focus on object detection, the most common and valued ones are Single Shot Detection (SSD) and Tensorflow. Both have an incredible inference process capability and the TFlite version demonstrated a great tool for embedded systems.\n",
    "Thus, with all the tools chosen, this paper intends to compare the combination of theses algorithmics for object detection solutions. This comparison aims to demonstrate how we can increase the video frame rate with simple approaches and demonstrate the best scenarios to handle each neural network algorithmics and the overlays plugins behavior on these tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materials and Methods\n",
    "\n",
    "This section describes the material such as the video files, models, labels, and programming language used, and the adopted methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming Language\n",
    "\n",
    "This paper uses Python 3 language and all the required support for the object detection algorithm: the overlays libs, GStreamer plugins, Tensorflow Lite, and SSD.\n",
    "* **OpenCV**: Open Source Computer Vision Library, this library is cross-platform and free for user under open-source BSD license. \n",
    "* **SVG image**: Scalable Vector Graphics, a Extensible Markup Language (XML)-based vector image format for two-dimensional graphics.  \n",
    "* **GStreamer**: GStreamer is a library for constructiong graphics of media-handling components and is released under the LGPL license.\n",
    "* **TFLite**: Tensoflow is a library for dataflow and differenctiable programming across of tasks, free and open-source under the Apache License 2.0. TFLite is a special version for mobile development. \n",
    "* **SSD**: Single Shot Detector algorithm designed for object detection in real-time applications.\n",
    "\n",
    "### Environment\n",
    "\n",
    "At the paper repository, in addition to the source code, the repository provides a DockerFile for local running. This Docker image provides to run this same experiment with all the required packages and its validated versions.\n",
    "It is important to alert that this study was made to be performed on embedded Linux systems, including dedicated hardware for inference process and GPU comparison, but it was adapted to run on iterated notebooks, where even basics process such as video playback does not work properly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models and Labels\n",
    "\n",
    "As the focus of this paper is the video techniques, and as the SSD and TFlite already have a huge numbers of pre-processed models, this paper will not care about pre-processing or training models and will uses pre-processed models available at the TFLite official object detection support web page.\n",
    "\n",
    "For this, the following pre-tested model and labels will be used at the tests:\n",
    "\n",
    "**Model**: mobilenet_ssd_v2_coco_quant_postprocess.tflite\n",
    "\n",
    "**Labels**: coco_labels.txt\n",
    "\n",
    "#### MobileNets\n",
    "\n",
    "MobileNets is a efficient models for mobile and embedded vision applications designed to effectively maximize accuracy while being mindful of the restricted resources for these devices.\n",
    "#### Common Objects in Context(COCO) Labels\n",
    "\n",
    "COCO is a large-scale object detection, segmentation, and captioning dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GStreamer and V4L2\n",
    "\n",
    "As mentioned before, this paper uses GStreamer framework to reproduce the videos files.\n",
    "For the comparison purpose, the following approaches will be performed:\n",
    "* OpenCV V4L2 directly handle;\n",
    "* GStreamer appsink pipeline + OpenCV V4L2 output;\n",
    "* GStreamer appsink + appsrc pipelines;\n",
    "* GStreamer overlay plugins support.\n",
    "\n",
    "The workflow below describes the difference between each process:\n",
    "\n",
    "### Workflow\n",
    "\n",
    "The most basic Object Detection workflow can be described as an input Layer (an image, for instance), being processed by some Hidden Layers (model), and resulting in an Ouput Layer (square over an object and a label on it). So basically a model was used to identify some objects in an image.\n",
    "\n",
    "However, this basic workflow will only work if the input layer has the same image size and color format expected for the model, i.e, in order to perform an Object Detection, an image pre-processing is required. Also, if it is expected to display the results in the same image, so an image pos-processing is required as well.\n",
    "\n",
    "To become it more complex, take a video file as source instead of an image, and before considering each frame as an image, the video will require a decoding process, and than the frames will be pre-processed, calculated the results by the model, pos-processed, and than displayed with the label results.\n",
    "\n",
    "Thus, we can split the Video Object Detection workflow into the following five steps:\n",
    "* 1- Input data;\n",
    "* 2- Image Pre-processing; \n",
    "* 3- Inference process;\n",
    "* 4- Image Pos-processing;\n",
    "* 5- Output data.\n",
    "\n",
    "**OpenCV V4L2 directly handle:**\n",
    "![opencv](../data/images/opencv_v4l2.png)\n",
    "The easiest way to perform Video Object Detection is by using OpenCV to perform the entire workflow process:\n",
    "* 1 - OpenCV uses Video4Linux (V4L2), a collection of device drivers and an API for supporting realtime video capture on Linux systems, which enable the user to pass a video file, image, or camera as Input Frames. OpenCV will handle it without any improvements.\n",
    "* 2 - OpenCV will take one-by-one Input Frame, pre-processing it, which includes color convert and image scaling, creating a Pre-processed Frame.\n",
    "* 3 - Here, the Pre-processed Frame is sent to the model, and then the inference process return the Resulted Frame.\n",
    "* 4 - The Resulted Frame is manipulated again by the OpenCV, drawing the squares, label, font colors, and etc. Then, the Resulted Frame is again scaled and applied a color convert, returning a Post-processed Frame.\n",
    "* 5 - In the end, OpenCV uses V4l2 to display the Post-processed Frame at the screen.\n",
    "\n",
    "The coast of this approach is the impossibility to improve the video capture/display, plus the fact of OpenCV has to handle all the five steps, being overloaded all the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrate the OpenCV V4L2 use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ObjectDetection import ObjectsDetectionOpenCV\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"This metod will take about 5 minutes to be finished.\")\n",
    "    app = ObjectsDetectionOpenCV()\n",
    "    app.run()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GStreamer appsink pipeline + OpenCV V4L2 output**\n",
    "![appsink](../data/images/appsink_opencv_v4l2.png)\n",
    "As mentioned before, one of the issues from the OpenCV approach is the impossibility to improve de video capture. So this new approach will uses a GStreamer pipeline in combination with OpenCV.VideoCapture function in order to improve the video input file. The GStreamer pipeline can be check below:\n",
    "```\n",
    "$ gst-launch-1.0 filesrc location=<video_file> ! qtdemux name=demux  demux.video_0 \\\n",
    "! queue ! decodebin ! queue max-size-buffers=1 ! videoconvert ! video/x-raw,format=BGR \\ \n",
    "! appsink sync=false drop=True max-buffers=1 emit-signals=True max-lateness=8000000000 \n",
    "```\n",
    "Altought only the step 1 was changed - and then minus one step to be processed by OpenCV - this change allow us to use some interesting properties, such as dropping frame capability, possibility to select the decoder plugin, and performing the videoconvert by GPU, when supported.\n",
    "\n",
    "Note that the steps 2 can be avoid as well, by using the videoconvert and videoscale plugins. However, as it only supports CPU, the OpenCV shows a better performance than it.\n",
    "\n",
    "As results, the displayed framerate will not be impacted for the inference process time or the decoding proces. However, the videoconvert usage, required for the appsink to be able to display the results at screen, is a disavantage, once its only supported by CPU, and resize/color convert by CPU has a high processing coast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrate this use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ObjectDetection import ObjectsDetectionV4L2\n",
    "\n",
    "\n",
    "def main():\n",
    "    #app = ObjectsDetectionV4L2()\n",
    "    #app.run()\n",
    "    print(\"This method is not supported on docker yet, so the values will be simulated. I am working to solve it.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GStreamer appsink + appsrc pipelines**\n",
    "![appsink/appsrc](../data/images/appsink_appsrc.png)\n",
    "\n",
    "Following the idea of GStreamer usage instead of OpenCV, this approach demonstrated the power of appsink and appsrc when used in combination.\n",
    "\n",
    "By using the same GStreamer appsink pipeline from the last example, but this time sending the Post-processed Frame to another GStreamer pipeline instead of OpenCV, allows us to improve not only the video capture, but the video displayed process as well:\n",
    "\n",
    "```\n",
    "$ gst-launch-1.0 filesrc location=<video_file> ! qtdemux name=demux  demux.video_0 \\\n",
    "! queue ! decodebin ! queue max-size-buffers=1 ! videoconvert ! video/x-raw,format=BGR \\ \n",
    "! appsink sync=false drop=True max-buffers=1 emit-signals=True max-lateness=8000000000 \n",
    "\n",
    "$ gst-launch-1.0 appsrc name=src is-live=True block=True \\\n",
    "! video/x-raw,format=RGB,width=640,height=480, \\\n",
    "framerate=30/1,interlace-mode=(string)progressive \\ \n",
    "! videoconvert ! ximagesink\n",
    "```\n",
    "\n",
    "Thus, the steps 1 and 5 are being processed by the GStreamer, and the OpenCV will handle only steps 2 and 4.\n",
    "\n",
    "Again, note that the steps 2 and 4 can be processed by GStreamer as well, but the OpenCV still shows a better performance than videoconvert and videoscale by CPU.\n",
    "\n",
    "However, on embedded systems, with GPU plugins support to color convert and scaling, this combination shows very promisingly. The properties to be applied are almost unlimited and each one can perform a different result, and even the sink plugin can be one supported by GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrate this use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ObjectDetection import ObjectsDetectionAppsrc\n",
    "\n",
    "\n",
    "def main():\n",
    "    #app = ObjectsDetectionAppsrc()\n",
    "    #app.run()\n",
    "    print(\"This method is not working properly on docker yet, so the values will be simulated. I am working to solve it.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GStreamer SVG overlay plugins support:**\n",
    "![overlay](../data/images/gstreamer_overlay.png)\n",
    "\n",
    "As show before, the image color convert and scaling can be an issue. Due to this, one aditional property can be applyed: overlay. As the name suggest, overlay is the capability to display some plane over other plane, and GStreamer has support for a lot of overlay plugins.\n",
    "\n",
    "With this approach, and by using rsvgoverlay plugin, this solution is able to take the Input Frame and send it to be processed and displayed at same time. So when it was processed, the overlay, after includes all the image details, as square, labels, etc, will overlay the display image with all the contents:\n",
    "\n",
    "\n",
    "```\n",
    "$ filesrc location=<video_file> ! qtdemux name=demux  demux.video_0 \\\n",
    "! queue ! decodebin  ! videorate ! videoconvert n-threads=4 ! videoscale n-threads=4 \\\n",
    "! video/x-raw,width={width},height={height},framerate=30/1 ! queue max-size-buffers=1 leaky=downstream \\\n",
    "! tee name=t \n",
    "    t. ! queue max-size-buffers=1 leaky=downstream ! videoconvert ! videoscale \\\n",
    "    ! video/x-raw,width={model_width},height={model_height ! videobox name=box autocrop=true\n",
    "    ! video/x-raw,format=RGB,width={width},height={height} \\\n",
    "    ! appsink name=appsink emit-signals=true max-buffers=1 drop=true sync=fals\n",
    "               \n",
    "    t. ! queue ! videoconvert\n",
    "    ! rsvgoverlay name=overlay ! videoconvert ! ximagesink\n",
    "```\n",
    "\n",
    "Here, the tee usage create two threads: one to be processed by the inference process; other to be displayed. The videobox keeps the frame and is able to return it ot the overlay plugin, so what we see is the combination of two frames, one is the original video, without be touched, other is an alpha image with all the required resizing process being displayed over the original video.\n",
    "\n",
    "With this approach, the video will never be impacted by the image pre-processing, inference, and image pos-processing steps. So the framerate will be the highest possible.\n",
    "\n",
    "And note again that all the plugins can be changed by the one with support to GPU, resulting in the best performance possible in terms of framerate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrate this use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ObjectDetection import ObjectsDetectionGStreamer\n",
    "\n",
    "def main():\n",
    "    app = ObjectsDetectionGStreamer()\n",
    "    app.run()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The tests consist in execute one video on all the purposed solutions, running an Object Detection TFlite by using SSD algorithm. In order to test and compare the performance of each solution, two values are being evaluated: the video framerate (FPS) and the inference time.\n",
    "\n",
    "The FPS presents the capability to display as many frames as possible in one unique second, while the inference time is the required elapsed time to get the frame, interpret it with TFLite, and return the objects founded to be displayed. The idea is to show that even for a slow inference time process, the user experience will not be affected and the displayed results will be smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below generates de values to compose the graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ObjectDetection import ObjectsDetectionOpenCV\n",
    "from ObjectDetection import ObjectsDetectionGStreamer\n",
    "\n",
    "fps = []\n",
    "inf = []\n",
    "\n",
    "opencv = ObjectsDetectionOpenCV()\n",
    "inf_opencv, fps_opencv = opencv.run()\n",
    "\n",
    "fps.append(fps_opencv)\n",
    "inf.append(inf_opencv)\n",
    "\n",
    "#V4L2 is still not working, so I am simulating a value\n",
    "fps.append(1.5)\n",
    "inf.append(490)\n",
    "\n",
    "#APPSRC is still not working, so I am simulating a value\n",
    "fps.append(2.25)\n",
    "inf.append(467)\n",
    "\n",
    "gstreamer = ObjectsDetectionGStreamer()\n",
    "inf_gstreamer, fps_gstreamer = gstreamer.run()\n",
    "\n",
    "fps.append(fps_gstreamer)\n",
    "inf.append(inf_gstreamer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below generates de FPS graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ObjectDetection import graphs\n",
    "\n",
    "fps_g = graphs()\n",
    "fps_g.start(fps, 'FPS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below generates de Inference graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ObjectDetection import graphs\n",
    "\n",
    "inf_g = graphs()\n",
    "inf_g.start(inf, 'Inference*1000 (ms)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Unfortunately, the notebooks have no performance enough even to handle playback videos, so one process as the object detection algorithms, which consumes even more process, was inviable to notebooks.\n",
    "Plus, according to the values obtained, noticed that the number of about 0.5 milliseconds is far from the expected for a video object detection solution. With some simples calculations, it is easy to note that 0.5 ms of inference time represents 2 frames being calculated per second. It is very slow and it is far from the multimedia capabilities, which can achieve 60 frames per second.\n",
    "However, one point was very clear: OpenCV V4L2 directly had a very poor performance than in comparison to the SVG Overlay performance, and this is exactly what this study intended to show, and all these solutions can be applied and enhanced with GPU supports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source\n",
    "\n",
    "Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. 2016. TensorFlow: A System for Large-Scale Machine Learning.\n",
    "\n",
    "Ganesh Ananthanarayanan, Paramvir Bahl, Peter Bodík, Krishna Chintalapudi, Matthai Philipose, Lenin Ravindranath, and Sudipta Sinha. 2017. Real-Time Video Analytics: The Killer App for Edge Computing.\n",
    "\n",
    "Google. 2019. Coral Edge TPU. Retrieved June 16, 2020, from https://coral.ai/\n",
    "\n",
    "Nvidia. 2020. NVIDIA DeepStream SDK. Retrieved June 16, 2020 from https://developer.nvidia.com/deepstream-sdk\n",
    "\n",
    "GStreamer. 2019. GStreamer: open source multimedia framework. Retrieved June 16, 2020 from https://gstreamer.freedesktop.org\n",
    "\n",
    "Intel. 2020.03. Intel's Deep Learning Inference Engine Developer Guide. Retrieved June 16, 2020 from https://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html\n"
   ]
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "6853877/6JUXZA65": {
     "DOI": "10.1162/106365600568202",
     "URL": "https://www.mitpressjournals.org/doi/10.1162/106365600568202",
     "abstract": "In this paper, we provide a systematic comparison of various evolutionary approaches to multiobjective optimization using six carefully chosen test functions. Each test function involves a particular feature that is known to cause difficulty in the evolutionary optimization process, mainly in converging to the Pareto-optimal front (e.g., multimodality and deception). By investigating these different problem features separately, it is possible to predict the kind of problems to which a certain technique is or is not well suited. However, in contrast to what was suspected beforehand, the experimental results indicate a hierarchy of the algorithms under consideration. Furthermore, the emerging effects are evidence that the suggested test functions provide sufficient complexity to compare multiobjective optimizers. Finally, elitism is shown to be an important factor for improving evolutionary multiobjective search.",
     "accessed": {
      "day": 16,
      "month": 4,
      "year": 2019
     },
     "author": [
      {
       "family": "Zitzler",
       "given": "Eckart"
      },
      {
       "family": "Deb",
       "given": "Kalyanmoy"
      },
      {
       "family": "Thiele",
       "given": "Lothar"
      }
     ],
     "container-title": "Evolutionary Computation",
     "container-title-short": "Evolutionary Computation",
     "id": "6853877/6JUXZA65",
     "issue": "2",
     "issued": {
      "day": 1,
      "month": 6,
      "year": 2000
     },
     "journalAbbreviation": "Evolutionary Computation",
     "page": "173-195",
     "page-first": "173",
     "shortTitle": "Comparison of Multiobjective Evolutionary Algorithms",
     "title": "Comparison of Multiobjective Evolutionary Algorithms: Empirical Results",
     "title-short": "Comparison of Multiobjective Evolutionary Algorithms",
     "type": "article-journal",
     "volume": "8"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
