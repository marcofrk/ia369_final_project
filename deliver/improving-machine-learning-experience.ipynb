{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Machine Learning Experience With GStreamer Techniques\n",
    "Marco A. Franchi\n",
    "\n",
    "## *Abstract*\n",
    "\n",
    "*It has being very common face machine learning algorithms at the multimedia area: traffic count; people count; real-time vigilance cameras; baggage treaker; face/expression recognition; and so on. However, it is perceptive the gap between machine learning and multimedia solutions, where even at simple embedded sistems it is possible to reach 4k videos running at 60 frames per second, whereas the best neural network solution is able to handle 224x224 frames at 300 milliseconds in the same embedded system. Due to this, a vast number of solutions were developed: dedicated hardware for inference process; models manipulation; accelerated pre-processing image solutions; and videos manupalation techniques. This paper is based at the study of these videos manipulation techniques, exposing the most common algorithmics, such as frame-skip, frame-droop, resizing, color convert, and overlay solutions; and preseting the benneficies and issues of the adoption or not of those techniques.*\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Aiming to diminish the gap between machine learning inference process time and the multimedia capability, which reachs 4k@60fps, some video manipulation solutions was purposed. Among them, the most common are the overlay solutions, which are able to create alpha layers over the video and insert information on it. These overlays are very common on object detections, once are responsible for drawing a square, select or color an object at the scene. The most common overlays are SVG, Cairo and OpenCV.\n",
    "As the overlays, it is important to use a great video framework able to handle all the elements involved at the inference and display solutions. One of the best and most usage one is the GStreamer framework. GStreamer is able to handle plugins in pipelines, which is perfect to do some tests very quickly.\n",
    "Appart from the video solutions, it is important to choose the best machine learning algorithmics as well. With focus on object detection, the most common and valueted ones are Single Shot Detection (SSD) and Tensorflow. Both as a incridible inference process capability, and the TFlite version demonstrated a great tool for embedded systems.\n",
    "Thus, with all the tools available, this paper intends to compare the combination of theses algorithmics for object detection solutions. This comparison aims to demonstrate how we can increase the video frame rate with simple approaches and demonstrate the best scenarios to handle each neural network algorithmics and the overlays plugins behavior on these tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materials and Methods\n",
    "\n",
    "This section describes the material such as the video files, models and programming language used, and adopted methodology.\n",
    "\n",
    "### Programming Language\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper uses Python 3 language and all the required support for the overlays, GStreamer, Tensorflow and SSD. These tools will be described in details on this section.\n",
    "As first test, it will uses opencv to open a video based file in a indepent new window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window=\"Video test\"\n",
    "opencv.namedWindow(window)\n",
    "file=\"../data/video/video_device.mp4\"\n",
    "cap=opencv.VideoCapture(file)\n",
    "    \n",
    "while (cap.isOpened()):\n",
    "    ret,frame=cap.read()\n",
    "        \n",
    "    if ret:\n",
    "        opencv.imshow(window,frame)\n",
    "        if opencv.waitKey(133)==27:\n",
    "            break\n",
    "    else :\n",
    "        break\n",
    "            \n",
    "opencv.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models and Labels\n",
    "\n",
    "As the focus of this paper is the video techniques, and as the YOLO, SSD and TFlite already have a huge numbers of pre-processed models, this paper will not care about pre-processing or training models.\n",
    "For this, the following pre-tested models and labels will be used at the tests:\n",
    "\n",
    "**Models**\n",
    "* tiny_yolov3.tflite\n",
    "* mobilenet_ssd_v2_coco_quant_postprocess.tflite\n",
    "* mobilenet_v2_1.0_224_quant.tflitetflite\n",
    "\n",
    "**Labels**\n",
    "* coco_labels.txt\n",
    "* labels.txt\n",
    "\n",
    "The following code section get all the required models and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "window=\"Video test\"\n",
    "opencv.namedWindow(window)\n",
    "file=\"../figures/video_device.mp4\"\n",
    "cap=opencv.VideoCapture(file)\n",
    "    \n",
    "while (cap.isOpened()):\n",
    "    ret,frame=cap.read()\n",
    "        \n",
    "    if ret:\n",
    "        opencv.imshow(window,frame)\n",
    "        if opencv.waitKey(133)==27:\n",
    "            break\n",
    "    else :\n",
    "        break\n",
    "            \n",
    "opencv.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GStreamer and v4l2\n",
    "\n",
    "As mentioned before, this paper uses GStreamer framework to reproduce the videos files.\n",
    "For the comparison purpose, the following approaches will be performed:\n",
    "* OpenCV v4l2 directly handle;\n",
    "* GStreamer appsink pipeline + OpenCV v4l2 output;\n",
    "* GStreamer appsink + appsrc pipelines;\n",
    "* GStreamer overlay plugins support.\n",
    "\n",
    "The workflow below describes the difference between each process:\n",
    "\n",
    "### Workflow\n",
    "\n",
    "**OpenCV v4l2 directly handle:**\n",
    "![opencv](../data/images/opencv_v4l2.png)\n",
    "For this approach, the OpenCV needs to handle the entire process. This is easely to test, but not fast for ML purpose, once the display results will always waiting for the processed frame.\n",
    "\n",
    "\n",
    "**GStreamer appsink pipeline + OpenCV v4l2 output**\n",
    "![appsink](../data/images/appsink_opencv_v4l2.png)\n",
    "This idea shows a better perfomance than OpenCV v4l2 due the possibily to use dropping frame property at the GStreamer pipeline. It means that the displayed frame rate will not be impacted for the inference process. However, the videoconvert usage, required for the appsink to be able to display the results at screen, is a disavantage, once its only support CPU, and resize/color convert by CPU has a high processing coast.\n",
    "\n",
    "\n",
    "**GStreamer appsink + appsrc pipelines**\n",
    "![appsink/appsrc](../data/images/appsink_appsrc.png)\n",
    "This combination shows very promissor, once we can use the appsink dropping frame property, but do not requires videoconvert, once its results will not be displayed yet. Actually, the resulted data will be processed by the inference process, which can include resizing and color convert, and the results will be send again to GStreamer, only to handle it and display at the screen.\n",
    "\n",
    "\n",
    "**GStreamer overlay plugins support:**\n",
    "![overlay](../data/images/gstreamer_overlay.png)\n",
    "\n",
    "This is the best approach, but the most dificult to use. Here, the tee usage create two threads: one to be processed by the inference process; other to be displayed. The videobox keeps the frame and is able to return it ot the overlay plugin, so what we see is the combination of two frames, one is the original video, without be touched, other is an alpha image with all the required resizing process being displayed over the original video.\n",
    "\n",
    "The following modularized source code is used to handle the four purposed solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opencv_v4l2():\n",
    "    window=\"Video test\"\n",
    "    opencv.namedWindow(window)\n",
    "    file=\"../data/video/video_device.mp4\"\n",
    "    #OpenCV handle the video file directly with VideoCapture v4l2 based function\n",
    "    cap=opencv.VideoCapture(file)\n",
    "\n",
    "    while (cap.isOpened()):\n",
    "        ret,frame=cap.read()\n",
    "\n",
    "        if ret:\n",
    "            #Opencv displays the content results by using v4l2 again\n",
    "            opencv.imshow(window,frame)\n",
    "            if opencv.waitKey(133)==27:\n",
    "                break\n",
    "        else :\n",
    "            break\n",
    "\n",
    "    opencv.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gstreamer_pipeline_opencv_v4l2():\n",
    "    window=\"Video test\"\n",
    "    opencv.namedWindow(window)\n",
    "    file=\"../data/video/video_device.mp4\"\n",
    "    #Uses GStreamer pipeline with leaky property, wich allows dropping frames\n",
    "    cap=opencv.VideoCapture(\"\"\"filesrc location=../data/video/video_device.mp4 ! decodebin !\n",
    "                            queue max-size-buffers=1 leaky=downstream ! videoconvert !\n",
    "                            appsink emit-signail=true max-buffers=1 drop=true\"\"\")\n",
    "\n",
    "    while (cap.isOpened()):\n",
    "        ret,frame=cap.read()\n",
    "\n",
    "        if ret:\n",
    "            #Opencv displays the content results by using v4l2 again\n",
    "            opencv.imshow(window,frame)\n",
    "            if opencv.waitKey(133)==27:\n",
    "                break\n",
    "        else :\n",
    "            break\n",
    "\n",
    "    opencv.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gstreamer_appsink_appsrc():\n",
    "    window=\"Video test\"\n",
    "    opencv.namedWindow(window)\n",
    "    pipeline1_cmd=\"filesrc location=../data/video/video_device.mp4 do-timestamp=True ! decodebin ! videoconvert ! \\\n",
    "        videoscale n-threads=4 method=nearest-neighbour ! \\\n",
    "        video/x-raw,format=RGB,width=1920,height=1080 ! \\\n",
    "        queue leaky=downstream max-size-buffers=1 ! appsink name=sink \\\n",
    "        drop=True max-buffers=1 emit-signals=True max-lateness=8000000000\"\n",
    "\n",
    "    pipeline2_cmd = \"appsrc name=appsource1 is-live=True block=True ! \\\n",
    "        video/x-raw,format=RGB,width=1920,height=1080, \\\n",
    "        framerate=20/1,interlace-mode=(string)progressive ! \\\n",
    "        videoconvert ! autovideosink\"\n",
    "\n",
    "    self.pipeline1 = Gst.parse_launch(pipeline1_cmd)\n",
    "    appsink = self.pipeline1.get_by_name('sink')\n",
    "    appsink.connect(\"new-sample\", self.on_new_frame, appsink)\n",
    "\n",
    "    self.pipeline2 = Gst.parse_launch(pipeline2_cmd)\n",
    "    self.appsource = self.pipeline2.get_by_name('appsource1')\n",
    "\n",
    "    self.pipeline1.set_state(Gst.State.PLAYING)\n",
    "    bus1 = self.pipeline1.get_bus()\n",
    "    self.pipeline2.set_state(Gst.State.PLAYING)\n",
    "    bus2 = self.pipeline2.get_bus()\n",
    "\n",
    "    while (cap.isOpened()):\n",
    "        ret,frame=cap.read()\n",
    "\n",
    "        if ret:\n",
    "            #Opencv displays the content results by using v4l2 again\n",
    "            opencv.imshow(window,frame)\n",
    "            if opencv.waitKey(133)==27:\n",
    "                break\n",
    "        else :\n",
    "            break\n",
    "\n",
    "    opencv.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gstreamer_overlay(overlay):\n",
    "    window=\"Video test\"\n",
    "    opencv.namedWindow(window)\n",
    "    scale = min(\n",
    "        appsink_size[0] /\n",
    "        src_size[0],\n",
    "        appsink_size[1] /\n",
    "        src_size[1])\n",
    "    scale = tuple(int(x * scale) for x in src_size)\n",
    "    scale_caps = 'video/x-raw,width={width},height={height}'.format(\n",
    "        width=scale[0], height=scale[1])\n",
    "    PIPELINE = 'filesrc location=%s ! decodebin' % videofile\n",
    "    PIPELINE += \"\"\" ! tee name=t\n",
    "         t. ! {leaky_q} ! imxvideoconvert_g2d ! {scale_caps} ! videobox name=box autocrop=true\n",
    "            ! {sink_caps} ! {sink_element}\n",
    "         t. ! queue ! imxvideoconvert_g2d\n",
    "            ! rsvgoverlay name=overlay ! waylandsink\n",
    "    \"\"\"\n",
    "    \n",
    "    SRC_CAPS = 'video/x-raw,width={width},height={height},framerate=30/1'\n",
    "    SINK_ELEMENT = 'appsink name=appsink emit-signals=true max-buffers=1 drop=true'\n",
    "    SINK_CAPS = 'video/x-raw,format=RGB,width={width},height={height}'\n",
    "    LEAKY_Q = 'queue max-size-buffers=1 leaky=downstream'\n",
    "\n",
    "    src_caps = SRC_CAPS.format(width=src_size[0], height=src_size[1])\n",
    "    sink_caps = SINK_CAPS.format(width=appsink_size[0], height=appsink_size[1])\n",
    "    pipeline = PIPELINE.format(leaky_q=LEAKY_Q,\n",
    "                               src_caps=src_caps, sink_caps=sink_caps,\n",
    "                               sink_element=SINK_ELEMENT, scale_caps=scale_caps)\n",
    "\n",
    "    while (cap.isOpened()):\n",
    "        ret,frame=cap.read()\n",
    "\n",
    "        if ret:\n",
    "            #Opencv displays the content results by using v4l2 again\n",
    "            opencv.imshow(window,frame)\n",
    "            if opencv.waitKey(133)==27:\n",
    "                break\n",
    "        else :\n",
    "            break\n",
    "\n",
    "    opencv.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlays\n",
    "\n",
    "The last modularized GStreamer code as the overlay property. This overlay will be used to display the following at the scree:\n",
    "* inference time\n",
    "* video FPS (Frame per second)\n",
    "* draw a square around the detected objects\n",
    "* put a label at these objects\n",
    "* confidence of it\n",
    "\n",
    "One example is displayed at the image below:\n",
    "\n",
    "![detection](../data/images/car_detection.gif)\n",
    "\n",
    "This overlays procces comparison will be very important for this paper results. So three overlays will be compared:\n",
    "* Opencv\n",
    "* Cairo\n",
    "* SVG\n",
    "\n",
    "We will use the source code below to use each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_opencv(self, opencv_im, objs, labels):\n",
    "    height, width, channels = opencv_im.shape\n",
    "    for obj in objs:\n",
    "        x0, y0, x1, y1 = list(obj.bbox)\n",
    "        x0, y0, x1, y1 = int(\n",
    "            x0 * width), int(\n",
    "            y0 * height), int(x1 * width), int(y1 * height)\n",
    "\n",
    "        percent = int(100 * obj.score)\n",
    "        label = '{}% {}'.format(percent, labels.get(obj.id, obj.id))\n",
    "\n",
    "        opencv_im = opencv.rectangle(\n",
    "            opencv_im, (x0, y0), (x1, y1), (0, 255, 0), 2)\n",
    "        opencv_im = opencv.putText(opencv_im, label, (x0, y0 + 30),\n",
    "                                   opencv.FONT_HERSHEY_SIMPLEX, 1.0,\n",
    "                                   (255, 0, 0), 2)\n",
    "        \n",
    "def generate_cairo(self, img, objs, labels):\n",
    "    stride = cairo.ImageSurface.format_stride_for_width(cairo.FORMAT_RGB24, width)\n",
    "    surface = cairo.ImageSurface.create_for_data(buffer, cairo.FORMAT_RGB24,\n",
    "                                                 width, height, stride)\n",
    "    context = cairo.Context(surface)\n",
    "    overlay = self.overlay()\n",
    "    x = width - overlay.get_width()\n",
    "    y = height - overlay.get_height()\n",
    "\n",
    "    context.set_source_surface(self.overlay(), x, y)\n",
    "    context.paint()\n",
    "\n",
    "def generate_svg(self, src_size, inference_size,\n",
    "                 inference_box, objs, labels, text_lines):\n",
    "    dwg = svgwrite.Drawing('', size=src_size)\n",
    "    src_w, src_h = src_size\n",
    "    inf_w, inf_h = inference_size\n",
    "    box_x, box_y, box_w, box_h = inference_box\n",
    "    scale_x, scale_y = src_w / box_w, src_h / box_h\n",
    "\n",
    "    for y, line in enumerate(text_lines, start=1):\n",
    "        self.shadow_text(dwg, 10, y * 20, line)\n",
    "    for obj in objs:\n",
    "        x0, y0, x1, y1 = list(obj.bbox)\n",
    "        x, y, w, h = x0, y0, x1 - x0, y1 - y0\n",
    "        x, y, w, h = int(x * inf_w), int(y *\n",
    "                                        inf_h), int(w * inf_w), int(h * inf_h)\n",
    "        x, y = x - box_x, y - box_y\n",
    "        x, y, w, h = x * scale_x, y * scale_y, w * scale_x, h * scale_y\n",
    "        percent = int(100 * obj.score)\n",
    "        label = '{}% {}'.format(percent, labels.get(obj.id, obj.id))\n",
    "        self.shadow_text(dwg, x, y - 5, label)\n",
    "        dwg.add(dwg.rect(insert=(x, y), size=(w, h),\n",
    "                        fill='none', stroke='red', stroke_width='2'))\n",
    "    return dwg.tostring()\n",
    "                        sink_element=SINK_ELEMENT, scale_caps=scale_caps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The following results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This is the conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source\n",
    "\n",
    "Hwang, C. & Yoon, K., 1981. Multiple Attributes Decision Making Methods and Applications. Berlin: Springer-Verlag."
   ]
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "6853877/6JUXZA65": {
     "DOI": "10.1162/106365600568202",
     "URL": "https://www.mitpressjournals.org/doi/10.1162/106365600568202",
     "abstract": "In this paper, we provide a systematic comparison of various evolutionary approaches to multiobjective optimization using six carefully chosen test functions. Each test function involves a particular feature that is known to cause difficulty in the evolutionary optimization process, mainly in converging to the Pareto-optimal front (e.g., multimodality and deception). By investigating these different problem features separately, it is possible to predict the kind of problems to which a certain technique is or is not well suited. However, in contrast to what was suspected beforehand, the experimental results indicate a hierarchy of the algorithms under consideration. Furthermore, the emerging effects are evidence that the suggested test functions provide sufficient complexity to compare multiobjective optimizers. Finally, elitism is shown to be an important factor for improving evolutionary multiobjective search.",
     "accessed": {
      "day": 16,
      "month": 4,
      "year": 2019
     },
     "author": [
      {
       "family": "Zitzler",
       "given": "Eckart"
      },
      {
       "family": "Deb",
       "given": "Kalyanmoy"
      },
      {
       "family": "Thiele",
       "given": "Lothar"
      }
     ],
     "container-title": "Evolutionary Computation",
     "container-title-short": "Evolutionary Computation",
     "id": "6853877/6JUXZA65",
     "issue": "2",
     "issued": {
      "day": 1,
      "month": 6,
      "year": 2000
     },
     "journalAbbreviation": "Evolutionary Computation",
     "page": "173-195",
     "page-first": "173",
     "shortTitle": "Comparison of Multiobjective Evolutionary Algorithms",
     "title": "Comparison of Multiobjective Evolutionary Algorithms: Empirical Results",
     "title-short": "Comparison of Multiobjective Evolutionary Algorithms",
     "type": "article-journal",
     "volume": "8"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
