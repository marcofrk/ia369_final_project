{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Machine Learning Experience With Multimedia Techniques\n",
    "Marco A. Franchi\n",
    "\n",
    "## *Abstract*\n",
    "\n",
    "*It has been very common face machine learning algorithms at the multimedia area: traffic count; real-time vigilance cameras; baggage tracker; face/expression recognition; and so on. It is so common that it was designed a name for it, Machine Vision. However, it is perceptive the gap between machine learning and multimedia solutions, where even at simple embedded systems it is possible to reach 4k videos running at 60 frames per second, whereas the best neural network solution only handles 224x224 frames at 300 milliseconds in the same system. Due to this gap, a vast number of solutions as being developed: dedicated hardware for inference process; model manipulation; accelerated pre-processing image solutions; and video manipulation techniques. This paper is based at the study of these videos manipulation techniques, exposing the most common algorithmics, such as frame-skip, frame-droop, resizing, color convert, and overlay solutions; and showing the main difference between them, when the solution can be applied, and the expected performance for each one.*\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Aiming to diminish the gap between the machine learning inference process and the multimedia capability, which reaches 4k@60fps, some video manipulation solutions was purposed. Among them, the most common is the overlay solutions, which are able to create alpha layers over the video and insert information on it. These overlays are very common on object detections algorithmics, once they are responsible for drawing a square, select or color an object at the scene. The most common overlays are Scalable Vector Graphics (SVG), Cairo, and OpenCV.\n",
    "In addition to the overlays, it is important to use a great video framework able to handle all the elements involved at the inference and display solutions. One of the best and most useful ones is the GStreamer framework. GStreamer is able to handle plugins in pipelines, which is perfect to do quick tests.\n",
    "Apart from the video solutions, it is important to choose the best machine learning algorithmics as well. With a focus on object detection, the most common and valued ones are Single Shot Detection (SSD) and Tensorflow. Both have an incredible inference process capability and the TFlite version demonstrated a great tool for embedded systems.\n",
    "Thus, with all the tools chosen, this paper intends to compare the combination of theses algorithmics for object detection solutions. This comparison aims to demonstrate how we can increase the video frame rate with simple approaches and demonstrate the best scenarios to handle each neural network algorithmics and the overlays plugins behavior on these tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materials and Methods\n",
    "\n",
    "This section describes the material such as the video files, models, labels, and programming language used, and the adopted methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming Language\n",
    "\n",
    "This paper uses Python 3 language and all the required support for the object detection algorithm: the overlays libs, GStreamer plugins, Tensorflow Lite, and SSD.\n",
    "* **OpenCV**: Open Source Computer Vision Library, this library is cross-platform and free for user under open-source BSD license. \n",
    "* **SVG image**: Scalable Vector Graphics, a Extensible Markup Language (XML)-based vector image format for two-dimensional graphics.  \n",
    "* **Cairo**: Cairo is a 2D graphics library with support for multiples output devices and is available under LGPLv2.1 license.\n",
    "* **GStreamer**: GStreamer is a library for constructiong graphics of media-handling components and is released under the LGPL license.\n",
    "* **TFLite**: Tensoflow is a library for dataflow and differenctiable programming across of tasks, free and open-source under the Apache License 2.0. TFLite is a special version for mobile development. \n",
    "* **SSD**: Single Shot Detector algorithm designed for object detection in real-time applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To guarantee support for video display, as a first test, the following code uses OpenCV to open and display a video-based file in new window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "window=\"Video test\"\n",
    "opencv.namedWindow(window)\n",
    "file=\"../data/video/Home-And-Away.mp4\"\n",
    "cap=opencv.VideoCapture(file)\n",
    "    \n",
    "while (cap.isOpened()):\n",
    "    ret,frame=cap.read()\n",
    "        \n",
    "    if ret:\n",
    "        opencv.imshow(window,frame)\n",
    "        if opencv.waitKey(133)==27: #Please, close it with ESC\n",
    "            break\n",
    "    else :\n",
    "        break\n",
    "\n",
    "opencv.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models and Labels\n",
    "\n",
    "As the focus of this paper is the video techniques, and as the SSD and TFlite already have a huge numbers of pre-processed models, this paper will not care about pre-processing or training models and will uses pre-processed models available at the TFLite official object detection support web page.\n",
    "\n",
    "For this, the following pre-tested model and labels will be used at the tests:\n",
    "\n",
    "**Model**: mobilenet_ssd_v2_coco_quant_postprocess.tflite\n",
    "\n",
    "**Labels**: coco_labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code section get all the required models and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ObjectsDetectionTFLiteSSD\n",
    "\n",
    "def main():\n",
    "    test = ObjectsDetectinoTFLiteSSD()\n",
    "    test.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GStreamer and v4l2\n",
    "\n",
    "As mentioned before, this paper uses GStreamer framework to reproduce the videos files.\n",
    "For the comparison purpose, the following approaches will be performed:\n",
    "* OpenCV v4l2 directly handle;\n",
    "* GStreamer appsink pipeline + OpenCV v4l2 output;\n",
    "* GStreamer appsink + appsrc pipelines;\n",
    "* GStreamer overlay plugins support.\n",
    "\n",
    "The workflow below describes the difference between each process:\n",
    "\n",
    "### Workflow\n",
    "\n",
    "**OpenCV v4l2 directly handle:**\n",
    "![opencv](../data/images/opencv_v4l2.png)\n",
    "For this approach, the OpenCV needs to handle the entire process. This is easely to test, but not fast for ML purpose, once the display results will always waiting for the processed frame.\n",
    "\n",
    "\n",
    "**GStreamer appsink pipeline + OpenCV v4l2 output**\n",
    "![appsink](../data/images/appsink_opencv_v4l2.png)\n",
    "This idea shows a better perfomance than OpenCV v4l2 due the possibily to use dropping frame property at the GStreamer pipeline. It means that the displayed frame rate will not be impacted for the inference process. However, the videoconvert usage, required for the appsink to be able to display the results at screen, is a disavantage, once its only support CPU, and resize/color convert by CPU has a high processing coast.\n",
    "\n",
    "\n",
    "**GStreamer appsink + appsrc pipelines**\n",
    "![appsink/appsrc](../data/images/appsink_appsrc.png)\n",
    "This combination shows very promissor, once we can use the appsink dropping frame property, but do not requires videoconvert, once its results will not be displayed yet. Actually, the resulted data will be processed by the inference process, which can include resizing and color convert, and the results will be send again to GStreamer, only to handle it and display at the screen.\n",
    "\n",
    "\n",
    "**GStreamer overlay plugins support:**\n",
    "![overlay](../data/images/gstreamer_overlay.png)\n",
    "\n",
    "This is the best approach, but the most dificult to use. Here, the tee usage create two threads: one to be processed by the inference process; other to be displayed. The videobox keeps the frame and is able to return it ot the overlay plugin, so what we see is the combination of two frames, one is the original video, without be touched, other is an alpha image with all the required resizing process being displayed over the original video.\n",
    "\n",
    "The following modularized source code is used to handle the four purposed solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opencv_v4l2():\n",
    "    window=\"Video test\"\n",
    "    opencv.namedWindow(window)\n",
    "    file=\"../data/video/video_device.mp4\"\n",
    "    #OpenCV handle the video file directly with VideoCapture v4l2 based function\n",
    "    cap=opencv.VideoCapture(file)\n",
    "\n",
    "    while (cap.isOpened()):\n",
    "        ret,frame=cap.read()\n",
    "\n",
    "        if ret:\n",
    "            #Opencv displays the content results by using v4l2 again\n",
    "            opencv.imshow(window,frame)\n",
    "            if opencv.waitKey(133)==27:\n",
    "                break\n",
    "        else :\n",
    "            break\n",
    "\n",
    "    opencv.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gstreamer_pipeline_opencv_v4l2():\n",
    "    window=\"Video test\"\n",
    "    opencv.namedWindow(window)\n",
    "    file=\"../data/video/video_device.mp4\"\n",
    "    #Uses GStreamer pipeline with leaky property, wich allows dropping frames\n",
    "    cap=opencv.VideoCapture(\"\"\"filesrc location=../data/video/video_device.mp4 ! decodebin !\n",
    "                            queue max-size-buffers=1 leaky=downstream ! videoconvert !\n",
    "                            appsink emit-signail=true max-buffers=1 drop=true\"\"\")\n",
    "\n",
    "    while (cap.isOpened()):\n",
    "        ret,frame=cap.read()\n",
    "\n",
    "        if ret:\n",
    "            #Opencv displays the content results by using v4l2 again\n",
    "            opencv.imshow(window,frame)\n",
    "            if opencv.waitKey(133)==27:\n",
    "                break\n",
    "        else :\n",
    "            break\n",
    "\n",
    "    opencv.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gstreamer_appsink_appsrc():\n",
    "    window=\"Video test\"\n",
    "    opencv.namedWindow(window)\n",
    "    pipeline1_cmd=\"filesrc location=../data/video/video_device.mp4 do-timestamp=True ! decodebin ! videoconvert ! \\\n",
    "        videoscale n-threads=4 method=nearest-neighbour ! \\\n",
    "        video/x-raw,format=RGB,width=1920,height=1080 ! \\\n",
    "        queue leaky=downstream max-size-buffers=1 ! appsink name=sink \\\n",
    "        drop=True max-buffers=1 emit-signals=True max-lateness=8000000000\"\n",
    "\n",
    "    pipeline2_cmd = \"appsrc name=appsource1 is-live=True block=True ! \\\n",
    "        video/x-raw,format=RGB,width=1920,height=1080, \\\n",
    "        framerate=20/1,interlace-mode=(string)progressive ! \\\n",
    "        videoconvert ! autovideosink\"\n",
    "\n",
    "    self.pipeline1 = Gst.parse_launch(pipeline1_cmd)\n",
    "    appsink = self.pipeline1.get_by_name('sink')\n",
    "    appsink.connect(\"new-sample\", self.on_new_frame, appsink)\n",
    "\n",
    "    self.pipeline2 = Gst.parse_launch(pipeline2_cmd)\n",
    "    self.appsource = self.pipeline2.get_by_name('appsource1')\n",
    "\n",
    "    self.pipeline1.set_state(Gst.State.PLAYING)\n",
    "    bus1 = self.pipeline1.get_bus()\n",
    "    self.pipeline2.set_state(Gst.State.PLAYING)\n",
    "    bus2 = self.pipeline2.get_bus()\n",
    "\n",
    "    while (cap.isOpened()):\n",
    "        ret,frame=cap.read()\n",
    "\n",
    "        if ret:\n",
    "            #Opencv displays the content results by using v4l2 again\n",
    "            opencv.imshow(window,frame)\n",
    "            if opencv.waitKey(133)==27:\n",
    "                break\n",
    "        else :\n",
    "            break\n",
    "\n",
    "    opencv.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gstreamer_overlay(overlay):\n",
    "    window=\"Video test\"\n",
    "    opencv.namedWindow(window)\n",
    "    scale = min(\n",
    "        appsink_size[0] /\n",
    "        src_size[0],\n",
    "        appsink_size[1] /\n",
    "        src_size[1])\n",
    "    scale = tuple(int(x * scale) for x in src_size)\n",
    "    scale_caps = 'video/x-raw,width={width},height={height}'.format(\n",
    "        width=scale[0], height=scale[1])\n",
    "    PIPELINE = 'filesrc location=%s ! decodebin' % videofile\n",
    "    PIPELINE += \"\"\" ! tee name=t\n",
    "         t. ! {leaky_q} ! imxvideoconvert_g2d ! {scale_caps} ! videobox name=box autocrop=true\n",
    "            ! {sink_caps} ! {sink_element}\n",
    "         t. ! queue ! imxvideoconvert_g2d\n",
    "            ! rsvgoverlay name=overlay ! waylandsink\n",
    "    \"\"\"\n",
    "    \n",
    "    SRC_CAPS = 'video/x-raw,width={width},height={height},framerate=30/1'\n",
    "    SINK_ELEMENT = 'appsink name=appsink emit-signals=true max-buffers=1 drop=true'\n",
    "    SINK_CAPS = 'video/x-raw,format=RGB,width={width},height={height}'\n",
    "    LEAKY_Q = 'queue max-size-buffers=1 leaky=downstream'\n",
    "\n",
    "    src_caps = SRC_CAPS.format(width=src_size[0], height=src_size[1])\n",
    "    sink_caps = SINK_CAPS.format(width=appsink_size[0], height=appsink_size[1])\n",
    "    pipeline = PIPELINE.format(leaky_q=LEAKY_Q,\n",
    "                               src_caps=src_caps, sink_caps=sink_caps,\n",
    "                               sink_element=SINK_ELEMENT, scale_caps=scale_caps)\n",
    "\n",
    "    while (cap.isOpened()):\n",
    "        ret,frame=cap.read()\n",
    "\n",
    "        if ret:\n",
    "            #Opencv displays the content results by using v4l2 again\n",
    "            opencv.imshow(window,frame)\n",
    "            if opencv.waitKey(133)==27:\n",
    "                break\n",
    "        else :\n",
    "            break\n",
    "\n",
    "    opencv.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlays\n",
    "\n",
    "The last modularized GStreamer code as the overlay property. This overlay will be used to display the following at the scree:\n",
    "* inference time\n",
    "* video FPS (Frame per second)\n",
    "* draw a square around the detected objects\n",
    "* put a label at these objects\n",
    "* confidence of it\n",
    "\n",
    "One example is displayed at the image below:\n",
    "\n",
    "![detection](../data/images/car_detection.gif)\n",
    "\n",
    "This overlays procces comparison will be very important for this paper results. So three overlays will be compared:\n",
    "* Opencv\n",
    "* Cairo\n",
    "* SVG\n",
    "\n",
    "We will use the source code below to use each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_opencv(self, opencv_im, objs, labels):\n",
    "    height, width, channels = opencv_im.shape\n",
    "    for obj in objs:\n",
    "        x0, y0, x1, y1 = list(obj.bbox)\n",
    "        x0, y0, x1, y1 = int(\n",
    "            x0 * width), int(\n",
    "            y0 * height), int(x1 * width), int(y1 * height)\n",
    "\n",
    "        percent = int(100 * obj.score)\n",
    "        label = '{}% {}'.format(percent, labels.get(obj.id, obj.id))\n",
    "\n",
    "        opencv_im = opencv.rectangle(\n",
    "            opencv_im, (x0, y0), (x1, y1), (0, 255, 0), 2)\n",
    "        opencv_im = opencv.putText(opencv_im, label, (x0, y0 + 30),\n",
    "                                   opencv.FONT_HERSHEY_SIMPLEX, 1.0,\n",
    "                                   (255, 0, 0), 2)\n",
    "        \n",
    "def generate_cairo(self, img, objs, labels):\n",
    "    stride = cairo.ImageSurface.format_stride_for_width(cairo.FORMAT_RGB24, width)\n",
    "    surface = cairo.ImageSurface.create_for_data(buffer, cairo.FORMAT_RGB24,\n",
    "                                                 width, height, stride)\n",
    "    context = cairo.Context(surface)\n",
    "    overlay = self.overlay()\n",
    "    x = width - overlay.get_width()\n",
    "    y = height - overlay.get_height()\n",
    "\n",
    "    context.set_source_surface(self.overlay(), x, y)\n",
    "    context.paint()\n",
    "\n",
    "def generate_svg(self, src_size, inference_size,\n",
    "                 inference_box, objs, labels, text_lines):\n",
    "    dwg = svgwrite.Drawing('', size=src_size)\n",
    "    src_w, src_h = src_size\n",
    "    inf_w, inf_h = inference_size\n",
    "    box_x, box_y, box_w, box_h = inference_box\n",
    "    scale_x, scale_y = src_w / box_w, src_h / box_h\n",
    "\n",
    "    for y, line in enumerate(text_lines, start=1):\n",
    "        self.shadow_text(dwg, 10, y * 20, line)\n",
    "    for obj in objs:\n",
    "        x0, y0, x1, y1 = list(obj.bbox)\n",
    "        x, y, w, h = x0, y0, x1 - x0, y1 - y0\n",
    "        x, y, w, h = int(x * inf_w), int(y *\n",
    "                                        inf_h), int(w * inf_w), int(h * inf_h)\n",
    "        x, y = x - box_x, y - box_y\n",
    "        x, y, w, h = x * scale_x, y * scale_y, w * scale_x, h * scale_y\n",
    "        percent = int(100 * obj.score)\n",
    "        label = '{}% {}'.format(percent, labels.get(obj.id, obj.id))\n",
    "        self.shadow_text(dwg, x, y - 5, label)\n",
    "        dwg.add(dwg.rect(insert=(x, y), size=(w, h),\n",
    "                        fill='none', stroke='red', stroke_width='2'))\n",
    "    return dwg.tostring()\n",
    "                        sink_element=SINK_ELEMENT, scale_caps=scale_caps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The following results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This is the conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source\n",
    "\n",
    "Hwang, C. & Yoon, K., 1981. Multiple Attributes Decision Making Methods and Applications. Berlin: Springer-Verlag."
   ]
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "6853877/6JUXZA65": {
     "DOI": "10.1162/106365600568202",
     "URL": "https://www.mitpressjournals.org/doi/10.1162/106365600568202",
     "abstract": "In this paper, we provide a systematic comparison of various evolutionary approaches to multiobjective optimization using six carefully chosen test functions. Each test function involves a particular feature that is known to cause difficulty in the evolutionary optimization process, mainly in converging to the Pareto-optimal front (e.g., multimodality and deception). By investigating these different problem features separately, it is possible to predict the kind of problems to which a certain technique is or is not well suited. However, in contrast to what was suspected beforehand, the experimental results indicate a hierarchy of the algorithms under consideration. Furthermore, the emerging effects are evidence that the suggested test functions provide sufficient complexity to compare multiobjective optimizers. Finally, elitism is shown to be an important factor for improving evolutionary multiobjective search.",
     "accessed": {
      "day": 16,
      "month": 4,
      "year": 2019
     },
     "author": [
      {
       "family": "Zitzler",
       "given": "Eckart"
      },
      {
       "family": "Deb",
       "given": "Kalyanmoy"
      },
      {
       "family": "Thiele",
       "given": "Lothar"
      }
     ],
     "container-title": "Evolutionary Computation",
     "container-title-short": "Evolutionary Computation",
     "id": "6853877/6JUXZA65",
     "issue": "2",
     "issued": {
      "day": 1,
      "month": 6,
      "year": 2000
     },
     "journalAbbreviation": "Evolutionary Computation",
     "page": "173-195",
     "page-first": "173",
     "shortTitle": "Comparison of Multiobjective Evolutionary Algorithms",
     "title": "Comparison of Multiobjective Evolutionary Algorithms: Empirical Results",
     "title-short": "Comparison of Multiobjective Evolutionary Algorithms",
     "type": "article-journal",
     "volume": "8"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
